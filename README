Setup:
There is one repo with the code:
git clone ssh://XXXX/congestion_control_code
and one with the VM images (this repo gets checked out into the vms directory in the first repo):
cd congestion_control_code/vms
git clone ssh://XXXX/congestion_control_vms/ .

Cloning the VMs may take a very long time (>30min) due to git not handling large files very well.

For reference, the username for all VMs is "root" and the password is "Gat11ng"

At this point you *may* need to adjust the permissions on the SSH private key:
chmod 600 config/ssh.key

Now you are ready to build the C components: the proxy, monitor, and state searcher. Simply type "make" in the root directory.

Next, choose a config file. There is a different config file for each implementation to test:
cd config
cp config.1110.py config.py

Now you can create the network bridges and tap devices:
./executor/network_setup.py start 1 2
(the numbers are whole instances of the test system, not per VM like turret)

At this point the system should be operational.


Running the System:

There are two components:

First, the coordinator (in terminal 1):
./coordinator/coordinator.py

Second, an executor (in terminal 2):
./executor/run.py -i 0 -c localhost
(-c uses the coordinator, -i sets the instance number)



Once the testing ends, you'll want to move the logs/ and captures/ directories to a safe location. I usually do:
mv logs logs.YYYMMDD
mv captures captures.YYYMMDD
git checkout logs/.placeholder
git checkout captures/.placeholder

which saves the output directories and resets the logs/ and captures/ directories.


Analysis:
To analyize the results, there are a couple scripts in the utils/ directory. utils/viewer.py walks through every report in the results.log file and allows you to easily see the strategies and results. It also gives you easy access to the details of the test (from inst*.log) and the captures.
utils/qv.py (quick view) parses the results.log and spits out something that is a little easier for humans to read
utils/timingGraph.py generates an output file that can be used with gnuplot and utils/timingGraph.gnu to produce a graph showing performance for each test (both time spent to download and amount of file downloaded).

While the inst*.log files can contain interesting info about the test, I've found that looking at the captures is the best way to figure out what's going on. I usually use tcptrace and xplot to examine the results. These tools graphically display a TCP connection and show you what's going on: when packets and acknowledgements were sent and what data they sent or acknowledge.



Network Diagram:
10.0.1.1                                  10.0.1.6            10.0.1.3
|                                          |                     |
Client 1                                 Monitor --------- Server 1 (Target Implementation)
(10.0.3.1)                            (eth1)  (eth2)      (10.0.3.3)
            \                         /
            br (eth1) Proxy  (eth2) br
            /            |           \
(10.0.3.2)               |           (10.0.3.4)
Client 2                 |             Server 2
|                        |                 |
10.0.1.2                 |             10.0.1.4
                         |
                      10.0.1.5

Each VM may have up to three IP addresses:
10.0.1.x which is connected to the host via the brhost bridge
10.0.3.x (where x is between 1 and 4). This address is actually connected to the testing system (think the ns-3 instance from turret).
10.0.2.x this is a NATed address to the global internet. I find myself wanting to install software (gdb, etc) for debugging and its a pain to not have this configured by default.

Note that 10.0.1.x addresses are unique to each VM while 10.0.3.x addresses are repeated for each instance (i.e. both instance 1 and instance 2 will have hosts with address 10.0.3.1---since they are completely isolated from each other this isn't a problem. Those machines would have addresses 10.0.1.3 and 10.0.1.9, respectively from the host).



FAQ:
Q)Sometimes I get this line "Main Traffic Command Failed! Return Code: XX", where XX is the return code. 
A)If you are running actual tests that's nothing to be concerned about. The return code being referred to is from the wget command we use to generate the connection we are manipulating. If that connection doesn't complete or times out, then you'll see the return code message. This usually occurs because the strategy being tested caused our connection to stall or become extremely slow. 

Q)How can I access the VM console, if something goes wrong
A)Use VNC. However, VNC is tied to the localhost to prevent it from being reachable from the global internet with no password. As a result, I usually use SSH port forwarding

Q)Can I start VMs manually before the test
A)You start the VMs manually with executor/manage_vms.py start <first_vm_num> <last_vm_num>. If you then run executor/run.py, it will emit a warning when it tries to start the VMs and they are already running, but shouldn't fail or crash. It should continue with the testing. For most VMs it will check that they are up and continue just fine.
